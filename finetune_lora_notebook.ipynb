{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "841dea07",
   "metadata": {},
   "source": [
    "# MoCo Model Fine-tuning with LoRA/QLoRA\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained MoCo model using LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) for parameter-efficient learning.\n",
    "\n",
    "**Key Benefits:**\n",
    "- üöÄ **Parameter Efficient**: Train only 1-5% of model parameters\n",
    "- üíæ **Memory Efficient**: Reduce memory footprint significantly\n",
    "- ‚ö° **Fast Training**: Quick adaptation to new tasks\n",
    "- üéØ **High Performance**: Maintain model quality while reducing computation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e585f1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba91eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Import custom modules\n",
    "sys.path.insert(0, '/Users/yeguo/VSCodeProjects/sem_moco')\n",
    "from model import MoCo\n",
    "from finetune_lora import LoRALayer, QLoRALayer, add_lora_to_model, freeze_backbone_params\n",
    "from lora_utils import LoRAManager, print_lora_config\n",
    "from utils import get_config_hierarchical\n",
    "from dataset import build_dataloader_from_dir, set_seed\n",
    "from torchvision import models\n",
    "\n",
    "print(\"‚úì Libraries imported successfully\")\n",
    "print(f\"‚úì PyTorch version: {torch.__version__}\")\n",
    "print(f\"‚úì GPU available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195da94",
   "metadata": {},
   "source": [
    "## 2. Load Pre-trained Model and Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Paths\n",
    "config_path = \"./configs/stage1.yaml\"\n",
    "checkpoint_path = \"./checkpoints/moco_stage1/moco_stage1_epoch_100.pth\"  # Update with your checkpoint path\n",
    "\n",
    "# Load config\n",
    "cfg = get_config_hierarchical(config_path)\n",
    "print(f\"‚úì Config loaded from {config_path}\")\n",
    "print(f\"  Image size: {cfg['img_size']}\")\n",
    "print(f\"  Batch size: {cfg['batch_size']}\")\n",
    "print(f\"  Proj dim: {cfg['proj_dim']}\")\n",
    "\n",
    "# Create base model\n",
    "print(\"\\nüèóÔ∏è  Creating MoCo model...\")\n",
    "backbone = models.resnet50(weights=None)\n",
    "model = MoCo(\n",
    "    backbone,\n",
    "    proj_dim=cfg[\"proj_dim\"],\n",
    "    hidden_dim=cfg[\"hidden_dim\"],\n",
    "    queue_size=cfg[\"queue_size\"],\n",
    "    momentum=cfg[\"momentum\"],\n",
    "    temperature=cfg[\"temperature\"],\n",
    ").to(device)\n",
    "\n",
    "# Load checkpoint\n",
    "if os.path.exists(checkpoint_path):\n",
    "    print(f\"üìÇ Loading checkpoint: {checkpoint_path}\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    print(\"‚úì Checkpoint loaded successfully\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Checkpoint not found: {checkpoint_path}\")\n",
    "    print(\"   Using randomly initialized weights\")\n",
    "\n",
    "print(f\"\\n‚úì Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2350f77",
   "metadata": {},
   "source": [
    "## 3. Prepare Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7047ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"üì¶ Loading datasets...\")\n",
    "train_loader = build_dataloader_from_dir(\n",
    "    cfg[\"data_path\"],\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    image_size=cfg[\"img_size\"],\n",
    "    split=\"train\",\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_loader = build_dataloader_from_dir(\n",
    "    cfg[\"data_path\"],\n",
    "    batch_size=cfg[\"batch_size\"],\n",
    "    image_size=cfg[\"img_size\"],\n",
    "    split=\"val\",\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train batches: {len(train_loader)}\")\n",
    "print(f\"‚úì Val batches: {len(val_loader)}\")\n",
    "\n",
    "# Inspect a batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "print(f\"\\nBatch info:\")\n",
    "print(f\"  Input 1 shape: {sample_batch[0].shape}\")\n",
    "print(f\"  Input 2 shape: {sample_batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7509502",
   "metadata": {},
   "source": [
    "## 4. Configure LoRA/QLoRA Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29aa1555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration Parameters\n",
    "lora_config = {\n",
    "    'rank': 8,              # LoRA rank (r) - smaller = fewer parameters\n",
    "    'alpha': 16,            # LoRA alpha - scaling factor\n",
    "    'target_modules': ['fc', 'linear'],  # Modules to apply LoRA to\n",
    "    'use_qlora': False,     # Set to True for QLoRA (4-bit quantization)\n",
    "    'freeze_backbone': True, # Freeze non-LoRA parameters\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in lora_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Explanation of parameters\n",
    "print(\"\\nüìñ Parameter Explanation:\")\n",
    "print(\"  ‚Ä¢ rank: Dimensionality of LoRA matrices (8-64 typical)\")\n",
    "print(\"  ‚Ä¢ alpha: Scaling factor (usually 2x rank)\")\n",
    "print(\"  ‚Ä¢ target_modules: Which layer types to apply LoRA\")\n",
    "print(\"  ‚Ä¢ use_qlora: 4-bit quantization for reduced memory\")\n",
    "print(\"  ‚Ä¢ freeze_backbone: Only train LoRA layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267f53d",
   "metadata": {},
   "source": [
    "## 5. Initialize LoRA/QLoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c22011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone model for LoRA version\n",
    "import copy\n",
    "model_lora = copy.deepcopy(model)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "print(\"üîß Adding LoRA layers...\")\n",
    "model_lora = add_lora_to_model(\n",
    "    model_lora,\n",
    "    r=lora_config['rank'],\n",
    "    lora_alpha=lora_config['alpha'],\n",
    "    target_modules=lora_config['target_modules'],\n",
    "    use_qlora=lora_config['use_qlora']\n",
    ")\n",
    "\n",
    "# Freeze backbone if needed\n",
    "if lora_config['freeze_backbone']:\n",
    "    print(\"‚ùÑÔ∏è  Freezing backbone parameters...\")\n",
    "    freeze_backbone_params(model_lora, freeze=True)\n",
    "\n",
    "# Print LoRA configuration\n",
    "print_lora_config(model_lora)\n",
    "\n",
    "# Create LoRA manager\n",
    "lora_manager = LoRAManager(model_lora)\n",
    "lora_info = lora_manager.get_lora_info()\n",
    "\n",
    "print(f\"\\nüìä LoRA Statistics:\")\n",
    "print(f\"  Total parameters: {lora_info['total_params']:,}\")\n",
    "print(f\"  LoRA parameters: {lora_info['total_lora_params']:,}\")\n",
    "print(f\"  LoRA ratio: {lora_info['lora_ratio']:.2f}%\")\n",
    "print(f\"  Number of LoRA layers: {len(lora_info['lora_layers'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878208a9",
   "metadata": {},
   "source": [
    "## 6. Set Up Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50cfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "training_config = {\n",
    "    'learning_rate': 1e-4,\n",
    "    'epochs': 5,\n",
    "    'batch_size': cfg['batch_size'],\n",
    "    'weight_decay': 1e-5,\n",
    "    'warmup_steps': 100,\n",
    "    'save_every_epochs': 1,\n",
    "    'use_amp': True,  # Automatic Mixed Precision\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*50)\n",
    "for key, value in training_config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Setup optimizer - only optimize LoRA parameters\n",
    "lora_params = [p for name, p in model_lora.named_parameters() if 'lora' in name]\n",
    "optimizer = torch.optim.AdamW(\n",
    "    lora_params,\n",
    "    lr=training_config['learning_rate'],\n",
    "    weight_decay=training_config['weight_decay']\n",
    ")\n",
    "\n",
    "# Setup loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Setup learning rate scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer,\n",
    "    T_max=training_config['epochs'],\n",
    "    eta_min=1e-6\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Optimizer created with {len(lora_params)} parameters\")\n",
    "print(f\"‚úì Learning rate scheduler configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51753374",
   "metadata": {},
   "source": [
    "## 7. Fine-tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e53901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(\"üöÄ Starting LoRA fine-tuning...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Setup for training\n",
    "model_lora.train()\n",
    "scaler = torch.cuda.amp.GradScaler() if training_config['use_amp'] else None\n",
    "\n",
    "# Store metrics\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Training epochs\n",
    "for epoch in range(training_config['epochs']):\n",
    "    print(f\"\\nüìç Epoch {epoch+1}/{training_config['epochs']}\")\n",
    "    \n",
    "    # Training phase\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (im_q, im_k) in enumerate(train_loader):\n",
    "        im_q = im_q.to(device)\n",
    "        im_k = im_k.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        if training_config['use_amp']:\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                logits, labels = model_lora(im_q, im_k)\n",
    "                loss = criterion(logits, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits, labels = model_lora(im_q, im_k)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Print progress\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            avg_loss = epoch_loss / num_batches\n",
    "            print(f\"  Batch {batch_idx+1}/{len(train_loader)} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Epoch metrics\n",
    "    avg_train_loss = epoch_loss / num_batches\n",
    "    train_losses.append(avg_train_loss)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    print(f\"  ‚úì Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "    \n",
    "    # Validation phase (optional, every epoch)\n",
    "    if True:  # Set to False to skip validation\n",
    "        model_lora.eval()\n",
    "        val_loss = 0.0\n",
    "        num_val_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for im_q, im_k in val_loader:\n",
    "                im_q = im_q.to(device)\n",
    "                im_k = im_k.to(device)\n",
    "                \n",
    "                if training_config['use_amp']:\n",
    "                    with torch.amp.autocast(device_type='cuda'):\n",
    "                        logits, labels = model_lora(im_q, im_k)\n",
    "                        loss = criterion(logits, labels)\n",
    "                else:\n",
    "                    logits, labels = model_lora(im_q, im_k)\n",
    "                    loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                num_val_batches += 1\n",
    "        \n",
    "        avg_val_loss = val_loss / num_val_batches\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"  ‚úì Validation Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        model_lora.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f0ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training curves\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "ax.plot(range(1, len(train_losses)+1), train_losses, 'b-o', label='Training Loss', linewidth=2)\n",
    "if val_losses:\n",
    "    ax.plot(range(1, len(val_losses)+1), val_losses, 'r-s', label='Validation Loss', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Loss', fontsize=12)\n",
    "ax.set_title('LoRA Fine-tuning: Training Progress', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"  Best train loss: {min(train_losses):.4f}\")\n",
    "if val_losses:\n",
    "    print(f\"  Best val loss: {min(val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda8b41",
   "metadata": {},
   "source": [
    "## 8. Save and Load Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39520b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA checkpoint\n",
    "output_dir = \"./checkpoints/lora\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(output_dir, \"moco_lora_finetuned.pth\")\n",
    "\n",
    "print(\"üíæ Saving LoRA checkpoint...\")\n",
    "lora_manager.save_lora_checkpoint(\n",
    "    checkpoint_path,\n",
    "    metadata={\n",
    "        'lora_rank': lora_config['rank'],\n",
    "        'lora_alpha': lora_config['alpha'],\n",
    "        'num_epochs': training_config['epochs'],\n",
    "        'final_train_loss': train_losses[-1] if train_losses else None,\n",
    "        'final_val_loss': val_losses[-1] if val_losses else None,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úì Checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "# Show checkpoint size comparison\n",
    "base_model_size = sum(p.numel() for p in model.parameters()) * 4 / (1024**2)\n",
    "lora_size = sum(p.numel() for name, p in model_lora.named_parameters() if 'lora' in name) * 4 / (1024**2)\n",
    "\n",
    "print(f\"\\nüìä Checkpoint Sizes:\")\n",
    "print(f\"  Base model: {base_model_size:.2f} MB\")\n",
    "print(f\"  LoRA only: {lora_size:.2f} MB\")\n",
    "print(f\"  Space saved: {100 * (1 - lora_size/base_model_size):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA checkpoint into a fresh model\n",
    "print(\"\\nüîÑ Loading LoRA checkpoint into fresh model...\")\n",
    "\n",
    "# Create fresh model\n",
    "fresh_model = MoCo(\n",
    "    models.resnet50(weights=None),\n",
    "    proj_dim=cfg[\"proj_dim\"],\n",
    "    hidden_dim=cfg[\"hidden_dim\"],\n",
    "    queue_size=cfg[\"queue_size\"],\n",
    "    momentum=cfg[\"momentum\"],\n",
    "    temperature=cfg[\"temperature\"],\n",
    ").to(device)\n",
    "\n",
    "# Load base checkpoint\n",
    "if os.path.exists(checkpoint_path.replace('moco_lora_finetuned.pth', '../moco_stage1/moco_stage1_epoch_100.pth')):\n",
    "    base_checkpoint = torch.load(\n",
    "        checkpoint_path.replace('moco_lora_finetuned.pth', '../moco_stage1/moco_stage1_epoch_100.pth'),\n",
    "        map_location=device\n",
    "    )\n",
    "    fresh_model.load_state_dict(base_checkpoint[\"model\"])\n",
    "\n",
    "# Add LoRA layers\n",
    "fresh_model = add_lora_to_model(\n",
    "    fresh_model,\n",
    "    r=lora_config['rank'],\n",
    "    lora_alpha=lora_config['alpha'],\n",
    "    target_modules=lora_config['target_modules'],\n",
    "    use_qlora=lora_config['use_qlora']\n",
    ")\n",
    "\n",
    "# Load LoRA weights\n",
    "fresh_lora_manager = LoRAManager(fresh_model)\n",
    "metadata = fresh_lora_manager.load_lora_checkpoint(checkpoint_path)\n",
    "\n",
    "print(f\"‚úì Model loaded successfully\")\n",
    "print(f\"‚úì Metadata: {metadata}\")\n",
    "\n",
    "# Set to evaluation mode\n",
    "fresh_model.eval()\n",
    "print(f\"‚úì Model in evaluation mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a171e266",
   "metadata": {},
   "source": [
    "## 9. Evaluate Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4497b64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: Compare base model vs LoRA fine-tuned model\n",
    "print(\"üìä Model Evaluation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def evaluate_model(model, val_loader, device, model_name=\"Model\"):\n",
    "    \"\"\"Evaluate model on validation set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for im_q, im_k in val_loader:\n",
    "            im_q = im_q.to(device)\n",
    "            im_k = im_k.to(device)\n",
    "            \n",
    "            logits, labels = model(im_q, im_k)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss\n",
    "\n",
    "# Evaluate base model\n",
    "print(f\"\\nüîç Evaluating base model...\")\n",
    "base_model.eval()\n",
    "base_loss = evaluate_model(model, val_loader, device, \"Base Model\")\n",
    "print(f\"  Base model val loss: {base_loss:.4f}\")\n",
    "\n",
    "# Evaluate LoRA fine-tuned model\n",
    "print(f\"\\nüîç Evaluating LoRA fine-tuned model...\")\n",
    "lora_loss = evaluate_model(model_lora, val_loader, device, \"LoRA Model\")\n",
    "print(f\"  LoRA model val loss: {lora_loss:.4f}\")\n",
    "\n",
    "# Compare results\n",
    "improvement = ((base_loss - lora_loss) / base_loss * 100)\n",
    "print(f\"\\nüìà Comparison:\")\n",
    "print(f\"  Base model loss: {base_loss:.4f}\")\n",
    "print(f\"  LoRA model loss: {lora_loss:.4f}\")\n",
    "print(f\"  Improvement: {improvement:.2f}%\")\n",
    "\n",
    "if improvement > 0:\n",
    "    print(f\"  ‚úì LoRA fine-tuning improved performance!\")\n",
    "else:\n",
    "    print(f\"  ‚ÑπÔ∏è  Further tuning may be needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573827cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary and next steps\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ LoRA Fine-tuning Summary\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nüìä Key Metrics:\")\n",
    "print(f\"  LoRA Rank: {lora_config['rank']}\")\n",
    "print(f\"  LoRA Alpha: {lora_config['alpha']}\")\n",
    "print(f\"  Trainable Parameters: {lora_info['total_lora_params']:,}\")\n",
    "print(f\"  Efficiency: {lora_info['lora_ratio']:.2f}% of total parameters\")\n",
    "print(f\"  Training Time: ~{training_config['epochs']} epochs\")\n",
    "print(f\"  Learning Rate: {training_config['learning_rate']}\")\n",
    "\n",
    "print(\"\\nüíæ Saved Artifacts:\")\n",
    "print(f\"  LoRA Checkpoint: {checkpoint_path}\")\n",
    "print(f\"  Size: {lora_size:.2f} MB (vs {base_model_size:.2f} MB base)\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Export the LoRA weights to ONNX format\")\n",
    "print(\"  2. Deploy the model with minimal overhead\")\n",
    "print(\"  3. Fine-tune on additional tasks by loading this checkpoint\")\n",
    "print(\"  4. Merge LoRA weights with base model for inference\")\n",
    "\n",
    "print(\"\\nüìö Useful Commands:\")\n",
    "print(\"  ‚Ä¢ lora_manager.merge_lora_weights() - Merge LoRA into base model\")\n",
    "print(\"  ‚Ä¢ lora_manager.save_lora_checkpoint() - Save LoRA weights\")\n",
    "print(\"  ‚Ä¢ lora_manager.load_lora_checkpoint() - Load LoRA weights\")\n",
    "print(\"  ‚Ä¢ lora_manager.get_lora_info() - Get LoRA statistics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
